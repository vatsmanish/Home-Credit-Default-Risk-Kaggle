{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:12:08.583171Z",
     "start_time": "2020-01-22T19:12:03.661949Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\MANDY\\\\Desktop\\\\fina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:40:50.129117Z",
     "start_time": "2020-01-22T17:40:46.073391Z"
    }
   },
   "outputs": [],
   "source": [
    "df2=pd.read_csv('application_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:01:53.955074Z",
     "start_time": "2020-01-22T17:01:53.945099Z"
    }
   },
   "outputs": [],
   "source": [
    "# df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:02:20.045679Z",
     "start_time": "2020-01-22T17:02:19.992013Z"
    }
   },
   "outputs": [],
   "source": [
    "# df2.AMT_REQ_CREDIT_BUREAU_QRT[(df2.AMT_REQ_CREDIT_BUREAU_QRT.isnull())]= 0\n",
    "# df2.AMT_REQ_CREDIT_BUREAU_HOUR[(df2.AMT_REQ_CREDIT_BUREAU_HOUR.isnull())]= 0\n",
    "# df2.AMT_REQ_CREDIT_BUREAU_YEAR[(df2.AMT_REQ_CREDIT_BUREAU_YEAR.isnull())]= 0\n",
    "# df2.AMT_REQ_CREDIT_BUREAU_DAY[(df2.AMT_REQ_CREDIT_BUREAU_DAY.isnull())]= 0\n",
    "# df2.AMT_REQ_CREDIT_BUREAU_MON[(df2.AMT_REQ_CREDIT_BUREAU_MON.isnull())]= 0\n",
    "# df2.AMT_REQ_CREDIT_BUREAU_WEEK[(df2.AMT_REQ_CREDIT_BUREAU_WEEK.isnull())]= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T16:47:17.896175Z",
     "start_time": "2020-01-22T16:46:48.413753Z"
    }
   },
   "outputs": [],
   "source": [
    "# df2.to_csv(\"train_imputed_done.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:13:02.170993Z",
     "start_time": "2020-01-22T19:12:08.629899Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'IMPUTED_previous_application.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-55a15563612c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# pos_cash_balance = pd.read_csv('POS_CASH_balance_imputed.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# bureau = pd.read_csv('bureau_imputed.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprevious_app\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IMPUTED_previous_application.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mapp_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_imputed_done.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mapp_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'application_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'IMPUTED_previous_application.csv' does not exist"
     ]
    }
   ],
   "source": [
    "bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "# pos_cash_balance = pd.read_csv('POS_CASH_balance_imputed.csv')\n",
    "# bureau = pd.read_csv('bureau_imputed.csv')\n",
    "previous_app = pd.read_csv('IMPUTED_previous_application.csv')\n",
    "app_train = pd.read_csv('train_imputed_done.csv')\n",
    "app_test = pd.read_csv('application_test.csv')\n",
    "credit_card_balance = pd.read_csv('credit_card_balance_imputed.csv')\n",
    "installments_payments = pd.read_csv('installments_payments_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:06:58.402123Z",
     "start_time": "2020-01-22T18:06:58.374159Z"
    }
   },
   "outputs": [],
   "source": [
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:13:04.732892Z",
     "start_time": "2020-01-22T19:13:04.295496Z"
    }
   },
   "outputs": [],
   "source": [
    "app_train.drop(\"Unnamed: 0\",inplace=True,axis=1)\n",
    "app_train.drop(\"Unnamed: 0.1\",inplace=True,axis=1)\n",
    "#app_train.drop(\" \",inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:19:22.664202Z",
     "start_time": "2020-01-22T18:19:13.837138Z"
    }
   },
   "outputs": [],
   "source": [
    "# bureau_balance = bureau_balance.loc[:,(bureau_balance.isnull().mean() < 0.2)]\n",
    "# pos_cash_balance = pos_cash_balance.loc[:,(pos_cash_balance.isnull().mean() < 0.2)]\n",
    "\n",
    "# bureau = bureau.loc[:,(bureau.isnull().mean() < 0.2)]\n",
    "# previous_app = previous_app.loc[:,(previous_app.isnull().mean() < 0.2)]\n",
    "# app_train = app_train.loc[:,(app_train.isnull().mean() < 0.2)]\n",
    "# #app_test = pd.read_csv('application_test.csv')\n",
    "# credit_card_balance = credit_card_balance.loc[:,(credit_card_balance.isnull().mean() < 0.2)]\n",
    "# installments_payments = installments_payments.loc[:,(installments_payments.isnull().mean() < 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:04:01.055103Z",
     "start_time": "2020-01-22T17:04:01.045130Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:13:07.466668Z",
     "start_time": "2020-01-22T19:13:06.279464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Groupby the client id (SK_ID_CURR), count the number of previous loans, and rename the column\n",
    "previous_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\n",
    "previous_loan_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:13:08.981894Z",
     "start_time": "2020-01-22T19:13:08.966274Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_categorical(df, group_var, col_name):\n",
    "    \n",
    "    \"\"\"Computes counts and normalized counts for each observation\n",
    "    of `group_var` for each unique category in every categorical variable\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df - DataFrame for which we will calculate count\n",
    "    \n",
    "    group_var  = string\n",
    "        The variable by which to group the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    col_name = string\n",
    "            Variable added to the front of column names to keep track of columns\n",
    "            \n",
    "            \"\"\"\n",
    "    # select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('object'))\n",
    "    \n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[group_var] = df[group_var]\n",
    "    \n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])                                              \n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['count', 'count_norm']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (col_name, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    return categorical\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:13:16.779506Z",
     "start_time": "2020-01-22T19:13:10.546597Z"
    }
   },
   "outputs": [],
   "source": [
    "bureau_counts = normalize_categorical(bureau, group_var = 'SK_ID_CURR', col_name = 'bureau')\n",
    "bureau_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:13:28.044995Z",
     "start_time": "2020-01-22T19:13:18.797208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grouping data  so  that we can merge all the files in 1 dataset\n",
    "\n",
    "data_bureau_agg=bureau.groupby(by='SK_ID_CURR').mean()\n",
    "data_credit_card_balance_agg=credit_card_balance.groupby(by='SK_ID_CURR').mean()\n",
    "data_previous_application_agg=previous_app.groupby(by='SK_ID_CURR').mean()\n",
    "data_installments_payments_agg=installments_payments.groupby(by='SK_ID_CURR').mean()\n",
    "data_POS_CASH_balance_agg=pos_cash_balance.groupby(by='SK_ID_CURR').mean()\n",
    "\n",
    "data_bureau_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:27:38.825750Z",
     "start_time": "2020-01-22T18:27:37.513558Z"
    }
   },
   "outputs": [],
   "source": [
    "#train = app_train.join(data_bureau_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n",
    "#train = app_train.join(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n",
    "# df = df.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n",
    "# df = df.join(data_credit_card_balance_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2')    \n",
    "# df = df.join(data_previous_application_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2')   \n",
    "# df = df.join(data_installments_payments_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2')\n",
    "\n",
    "count_nan = (len(train) - train.count())*100/len(train)\n",
    "count_nan.to_excel(\"null3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:25:07.903908Z",
     "start_time": "2020-01-22T18:25:07.888289Z"
    }
   },
   "outputs": [],
   "source": [
    "data_bureau_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:27:09.392845Z",
     "start_time": "2020-01-22T18:27:09.346285Z"
    }
   },
   "outputs": [],
   "source": [
    "bureau_counts.head()\n",
    "#data_bureau_agg.drop(\"Unnamed: 0\",inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:23:21.666306Z",
     "start_time": "2020-01-22T18:23:21.431939Z"
    }
   },
   "outputs": [],
   "source": [
    "count_nan = (len(data_bureau_agg) - data_bureau_agg.count())*100/len(data_bureau_agg)\n",
    "count_nan.to_excel(\"null4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:21:56.450039Z",
     "start_time": "2020-01-22T18:21:56.419247Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:13:37.284811Z",
     "start_time": "2020-01-22T19:13:34.065805Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge(df):\n",
    "    df = df.join(data_bureau_agg, how='inner', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n",
    "    df = df.join(bureau_counts, on = 'SK_ID_CURR', how = 'inner')\n",
    "    df = df.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'inner')\n",
    "    df = df.join(data_credit_card_balance_agg, how='inner', on='SK_ID_CURR', lsuffix='1', rsuffix='2')    \n",
    "    df = df.join(data_previous_application_agg, how='inner', on='SK_ID_CURR', lsuffix='1', rsuffix='2')   \n",
    "    df = df.join(data_installments_payments_agg, how='inner', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n",
    "    \n",
    "    return df\n",
    "\n",
    "train = merge(app_train)\n",
    "#test = merge(app_test)\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:32:27.025516Z",
     "start_time": "2020-01-22T18:32:27.010294Z"
    }
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T19:14:03.313123Z",
     "start_time": "2020-01-22T19:13:47.133601Z"
    }
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"merge_done_with_null.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:57:34.003936Z",
     "start_time": "2020-01-22T17:57:32.862738Z"
    }
   },
   "outputs": [],
   "source": [
    "count_nan = (len(train) - train.count())*100/len(train)\n",
    "count_nan.to_excel(\"null3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:50:26.248268Z",
     "start_time": "2020-01-22T18:50:26.138663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a label encoder object\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in train:\n",
    "    if train[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(train[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(train[col])\n",
    "            # Transform both training and testing data\n",
    "            train[col] = le.transform(train[col])\n",
    "            #test[col] = le.transform(test[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "\n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:50:56.824563Z",
     "start_time": "2020-01-22T18:50:56.605831Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(train)\n",
    "\n",
    "display(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:57:36.981983Z",
     "start_time": "2020-01-22T17:57:35.085036Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:48:47.391315Z",
     "start_time": "2020-01-22T18:48:47.360066Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:51:23.342151Z",
     "start_time": "2020-01-22T18:51:23.045346Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(all_data.drop(columns='TARGET'), all_data['TARGET'], test_size = 0.2, random_state = 200)\n",
    "print(\"X Training shape\", X_train.shape)\n",
    "print(\"X Testing shape\", X_test.shape)\n",
    "print(\"Y Training shape\", Y_train.shape)\n",
    "print(\"Y Testing shape\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:51:29.918784Z",
     "start_time": "2020-01-22T18:51:27.247536Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logreg = LogisticRegression(random_state=0, class_weight='balanced', C=100)\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "#Y_pred_proba = logreg.predict_proba(X_test)\n",
    "\n",
    "print('Train/Test split results:')\n",
    "#print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(Y_test, Y_pred))\n",
    "print(\"ROC\",  roc_auc_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
